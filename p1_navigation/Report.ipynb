{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Deep_Q_learning(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    \"\"\" Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)  \n",
    "        scores.append(score)     \n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that implements the Deep Q-Learning algorithm. It takes several hyperparameters as input, including the number of episodes to train on (n_episodes), the maximum number of timesteps per episode (max_t), the starting value of epsilon (eps_start), the minimum value of epsilon (eps_end), and the multiplicative factor for decreasing epsilon (eps_decay).\n",
    "\n",
    "## Hyperparamters\n",
    "\n",
    "\n",
    "eps_start=1.0 : Epsilon-Greedy ϵ start value\n",
    "\n",
    "eps_end=0.01 : Epsilon-Greedy ϵ end value\n",
    "\n",
    "eps_decay=0.995 : Epsilon-Greedy ϵ decay value\n",
    "\n",
    "\n",
    "eps_start, eps_end, eps_decay: These hyperparameters control the behavior of the epsilon-greedy action selection. eps_start is the initial value of epsilon, eps_end is the minimum value of epsilon, and eps_decay determines how quickly epsilon decreases over time. Epsilon determines the probability with which the agent will take a random action, as opposed to the action with the highest estimated Q-value. A high value of epsilon means the agent will take more random actions, while a low value means it will take fewer. The goal is for epsilon to gradually decrease over time, so that the agent becomes more confident in its actions as it gains more experience.\n",
    "\n",
    "\n",
    "\n",
    "The function starts by initializing a list to store the scores obtained in each episode, and a deque to store the last 100 scores. The epsilon value is initialized to eps_start. The function then enters a loop, where it performs the following steps for each episode:\n",
    "\n",
    "The environment is reset, and the initial state is obtained.\n",
    "The agent selects an action using the epsilon-greedy policy.\n",
    "The environment is stepped using the selected action, and the next state, reward, and done flag are obtained.\n",
    "The agent updates its Q-network using the information from the current transition.\n",
    "The state is updated to the next state, and the score is updated by adding the reward obtained.\n",
    "If the episode has ended, the loop is broken.\n",
    "The score for the current episode is added to the list of scores and appended to the scores_window deque.\n",
    "The epsilon value is updated by multiplying it with eps_decay, and taking the maximum with eps_end.\n",
    "If the episode is a multiple of 100, the average score over the last 100 episodes is printed.\n",
    "If the average score over the last 100 episodes is greater than or equal to 13.0, the function saves the agent's Q-network and returns the scores.\n",
    "The function returns the list of scores obtained in each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "\n",
    "There are many ways to improve this DQN implementation. Some possible enhancements are:\n",
    "\n",
    "Double DQN: This is a modification of DQN that helps to mitigate the over-estimation problem in DQN. It uses two separate networks, one for selecting actions and one for evaluating their values.\n",
    "\n",
    "Dueling DQN: This architecture splits the value and advantage streams and combines them in the final layers to obtain the Q-value for each action.\n",
    "\n",
    "Prioritized Experience Replay: In standard experience replay, the transition samples are uniformly sampled from the replay buffer. However, in prioritized experience replay, the samples are drawn with probabilities proportional to the absolute TD error of each transition. This helps to prioritize the learning of transitions with higher TD error, which are generally more important.\n",
    "\n",
    "Multi-step Learning: In this method, the agent considers multiple future steps, rather than just the next state, when estimating the target Q-value. This helps to propagate the reward information more efficiently.\n",
    "\n",
    "Noisy Networks: In this method, a Gaussian noise is added to the network's weights at each update step, to promote exploration and to prevent the network from getting stuck in a local optimum.\n",
    "\n",
    "Rainbow: Rainbow is an extension of DQN that combines several improvements, such as double DQN, dueling DQN, prioritized experience replay, multi-step learning, and noisy networks. It's considered to be the state-of-the-art algorithm for deep reinforcement learning problems.\n",
    "\n",
    "These are just a few possible improvements. It's important to keep in mind that the best approach for any particular problem will depend on the specifics of the task, and will require some experimentation to determine the optimal combination of techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
